{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dxd_WvaJOgV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "484acdbd-3fbc-4add-948e-6b6a5f0c6d07"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bc044f32678c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#!pip install dice_ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m  \u001b[0;31m# pyright: ignore # noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from pandas.compat.numpy import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnp_version_under1p21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# numpy versioning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pyright: reportUnusedImport = false\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.util._decorators import (  # noqa:F401\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m from pandas._typing import (\n\u001b[1;32m     16\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_libs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[1;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_libs/interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Libraries to install if not done before, restart kernel after installation\n",
        "#!pip install shap\n",
        "#!pip install lime\n",
        "#!pip install dice_ml\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Preparación de datos\n",
        "from sklearn.metrics import mutual_info_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# modelo elegido Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# análisis de rendimiento\n",
        "from sklearn.metrics import accuracy_score, f1_score, auc, recall_score, precision_score\n",
        "from sklearn.metrics import make_scorer, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
        "\n",
        "# XAI methods\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.inspection import partial_dependence\n",
        "import shap\n",
        "import lime\n",
        "import dice_ml as dice\n",
        "\n",
        "# model evaluation\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# calibración\n",
        "from sklearn.metrics import log_loss, brier_score_loss\n",
        "\n",
        "# gráficos\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# ignorar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"diabetes.csv\")\n",
        "df_copia = df.copy()\n",
        "df"
      ],
      "metadata": {
        "id": "jT5l0kOAPu_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_values = {\n",
        "    0: 'N', # Negativo\n",
        "    1: 'P', # Positivo\n",
        "}\n",
        "\n",
        "df[\"Outcome\"] = df[\"Outcome\"].map(new_values) #Sustituyo los valores enteros por categóricos\n",
        "df"
      ],
      "metadata": {
        "id": "G3qTXJTvaBOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "BFL1_S_1uuQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "87V_U-YSQTZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZIc9P6CERCA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Nº de filas duplicadas: {df.duplicated().sum()}\") #Duplicated devuelve True o False, sum cuenta cuantas veces hay True, es decir, no hay duplicados\n",
        "\n",
        "categoricas = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numericas = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "print(f\"Columnas categóricas: {categoricas}\")\n",
        "print(f\"Columnas numéricas: {numericas}\")"
      ],
      "metadata": {
        "id": "RlM5NLSBUbAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puede verse que en columnas como BloodPressure, BMI, Glucose, Insulin, SkinThicknes el valor mínimo es 0, esto es imposible, lo más probable es que se deba a haber sustituido valores nulls por 0, por lo que voy a eliminar todas las filas en las que algún valor de estas columnas sea  == 0.\n"
      ],
      "metadata": {
        "id": "OZjtUKstuEYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "3vPs7r5IUa9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df == 0].count()"
      ],
      "metadata": {
        "id": "Uyz5DaPxUa7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df[(df['Insulin'] != 0) & (df['BMI'] != 0) & (df['Glucose'] != 0) & (df['BloodPressure'] != 0) & (df['SkinThickness'] != 0)]\n",
        "df_clean"
      ],
      "metadata": {
        "id": "J2kYOUJFux7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.describe()"
      ],
      "metadata": {
        "id": "m99OOWE4vhKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIMPIEZA DE DATOS FINALIZADA"
      ],
      "metadata": {
        "id": "nw03dzpuZEZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANALISIS DE VARIABLES CATEGÓRICAS"
      ],
      "metadata": {
        "id": "j9b7QLa1723L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataframe, no posee variables categóricas más allá del Outcome que he convertido en categórica manualmente. Por lo que no puede hacerse el mismo estudio que se les va a hacer a las numéricas."
      ],
      "metadata": {
        "id": "AcvLotwe76UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale = 1)\n",
        "sns.countplot(x=\"Outcome\", data=df_clean).set(ylabel = \"Data count\"\n",
        "            , xlabel = \"Negativo                                   Positivo\")\n",
        "plt.yticks(ticks=range(0, df_clean['Outcome'].value_counts().max() + 50, 50))\n",
        "plt.title('Target distribution')"
      ],
      "metadata": {
        "id": "h-xIf3HNUa5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESTE ES ANTES DE HACER NINGUNA LIMPIEZA"
      ],
      "metadata": {
        "id": "JWTRRkhY3MBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale = 1)\n",
        "sns.countplot(x=\"Outcome\", data=df_copia).set(ylabel = \"Data count\"\n",
        "            , xlabel = \"Positivo                                   Negativo\")\n",
        "plt.yticks(ticks=range(0, df_copia['Outcome'].value_counts().max() + 50, 50))\n",
        "plt.title('Target distribution')"
      ],
      "metadata": {
        "id": "Rr7_Nz-KUa21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANALISIS DE VARIABLES NUMERICAS"
      ],
      "metadata": {
        "id": "scYqo4ck7ygm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This visualization takes a while, set the 'if' as \"True\" to run it\n",
        "if True:#False:\n",
        "    colors = [\"b\",\"r\"]\n",
        "    plt.figure(figsize=(30,50))\n",
        "    sns.set(font_scale = 2.5)\n",
        "\n",
        "    ax = sns.pairplot(df_clean, hue='Outcome', palette=colors, height=3, aspect = 1.9,\n",
        "                      kind=\"kde\")\n",
        "    ax.fig.suptitle('Pair Plots to show potential relations between numerical features (double-click to enlarge)',\n",
        "                size=40, ha='center', y = 1.05)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EojGTLLCUaqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_corr = df_clean.drop(columns=['Outcome']).corr().round(3)"
      ],
      "metadata": {
        "id": "Myvse3mWUaoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,10))\n",
        "sns.set(font_scale = 2)\n",
        "\n",
        "sns.heatmap(matrix_corr,annot=True,linewidths=.5, cmap=\"Blues\")\n",
        "plt.title('Heatmap showing correlations between numerical data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oNTWWBoBUald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí se hace un análisis de la importancia de cada variable numérica, como puede verse, la principal es la glucosa, seguida por la edad y la insulina."
      ],
      "metadata": {
        "id": "afzGi54K7oB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_scores = []\n",
        "\n",
        "for col in numericas:\n",
        "    auc = roc_auc_score(df_clean['Outcome'], df_clean[col])\n",
        "    if auc < 0.5: # in case the feature is negatively correlated with the target\n",
        "        auc = roc_auc_score(df_clean['Outcome'], -df_clean[col])\n",
        "    feature_scores.append((col, auc))\n",
        "\n",
        "columns = ['feature', 'ROC_AUC']\n",
        "df_scores = pd.DataFrame(feature_scores, columns=columns)\n",
        "df_scores.sort_values(by=['ROC_AUC'],ascending=False).reset_index(drop = True)"
      ],
      "metadata": {
        "id": "upi4YhOjUajE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_values = {\n",
        "    'N': 0, # Negativo\n",
        "    'P': 1, # Positivo\n",
        "}\n",
        "\n",
        "df_clean[\"Outcome\"] = df_clean[\"Outcome\"].map(new_values) #Sustituyo los valores enteros por categóricos\n",
        "\n",
        "df_select = df_clean.copy()\n",
        "\n",
        "categoricas = df_select.select_dtypes(include=['object']).columns.tolist()\n",
        "numericas = df_select.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "numericas = numericas[:-1] #Quito el Outcome\n",
        "print(f\"Columnas categóricas: {categoricas}\") #Este dataset no tiene columnas categóricas, por lo que no hará falta tratarlas\n",
        "print(f\"Columnas numéricas: {numericas}\")"
      ],
      "metadata": {
        "id": "GZrnVOmkUaeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar en entrenamiento y validación y test\n",
        "df_full_train, df_test = train_test_split(df_select, test_size=0.2, random_state=1)\n",
        "\n",
        "# Separar entrenamiento y validación, que debe ser un 20% del 80%\n",
        "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
        "\n",
        "len(df_train), len(df_val), len(df_test)"
      ],
      "metadata": {
        "id": "EvgxEquzUacC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Con esto como he eliminado filas, reinicio los indices\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "\n",
        "y_train = df_train['Outcome'].values\n",
        "y_test = df_test['Outcome'].values\n",
        "y_val = df_val['Outcome'].values"
      ],
      "metadata": {
        "id": "WW2peTr5UaZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARIABLES NUMERICAS ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "gacX8IyUaFKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# TRAIN\n",
        "X_train = df_train[numericas].values\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# VAL\n",
        "X_val = df_val[numericas].values\n",
        "X_val = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "NhPX1vczS2dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "FFItPXM6cefw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "id": "99IdeExTceZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGRESION LOGISTICA"
      ],
      "metadata": {
        "id": "gI4j5Gq3cQid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "Hrrk1cE-dok1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression(random_state= 1) # default solver is 'lbfgs' and regularization 'C=1'\n",
        "\n",
        "LR.fit(X_train, y_train)\n",
        "y_pred = LR.predict_proba(X_val)[:, 1]\n",
        "# the output is a matrix:\n",
        "# left column is the results for neg ('0', no heart failure), right column is for pos ('1', heart failure)\n",
        "\n",
        "# preliminary threshold to convert the raw score into a probability:\n",
        "t = 0.5  # above this threshold the raw score becomes 1, below, is zero"
      ],
      "metadata": {
        "id": "7LKastP_cQXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preliminary performance metric\n",
        "acc = accuracy_score(y_val, y_pred >= t)\n",
        "# it compares the 0 or 1 in y_val with the False or True of y_pred>=t\n",
        "\n",
        "print('thres', 'acc')\n",
        "print('%.2f %.3f' % (t, acc))"
      ],
      "metadata": {
        "id": "661U0TYkcOF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters grid\n",
        "LR_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    # the smaller the C the stronger the regularization, default is C=1\n",
        "    'max_iter': [50, 100, 200],\n",
        "    # default is 100, max iterations taken for the solver to converge to the min\n",
        "    'solver': ['lbfgs'],\n",
        "    # the default solver\n",
        "    'penalty': ['none', 'l2']\n",
        "    # default is 'l2'\n",
        "}\n",
        "\n",
        "# metric\n",
        "metric = make_scorer(f1_score)\n",
        "\n",
        "# model definition\n",
        "LR_grid = GridSearchCV(\n",
        "            estimator = LogisticRegression(random_state= 1),\n",
        "            param_grid = LR_param_grid,\n",
        "            refit = True,\n",
        "            verbose = 1,\n",
        "            cv = 5,  # the default is 5 for a 5-fold cross validation\n",
        "            scoring = metric\n",
        ")\n",
        "\n",
        "# train the model for grid search\n",
        "LR_grid.fit(X_train,y_train)\n",
        "LR_grid.best_params_\n",
        "\n",
        "# summarize\n",
        "print('Mean F1-score: %.3f' % LR_grid.best_score_)\n",
        "print('Standar Deviation:', LR_grid.cv_results_['std_test_score'][LR_grid.best_index_].round(3))\n",
        "print('Best Parameters: %s' % LR_grid.best_params_)"
      ],
      "metadata": {
        "id": "ePLsUJTDcOD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elección del mejor modelo"
      ],
      "metadata": {
        "id": "KKpY49n5hTxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression(random_state= 1, max_iter =50)\n",
        "LR.fit(X_train, y_train)\n",
        "y_pred = LR.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "3oPkf54UaWIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function that calculates the metrics for several thresholds\n",
        "\n",
        "def tune_threshold(y_val, y_pred, number_of_thres):\n",
        "\n",
        "    thresholds = np.linspace(0, 1, number_of_thres)\n",
        "    metrics =[]\n",
        "\n",
        "    for t in thresholds:\n",
        "        acc = accuracy_score(y_val, y_pred >= t)\n",
        "        pr  = precision_score(y_val, y_pred >= t,zero_division=0)\n",
        "        rec = recall_score(y_val, y_pred >= t)\n",
        "        f1  = f1_score(y_val, y_pred>=t)\n",
        "        metrics.append((t, acc, pr, rec, f1))\n",
        "\n",
        "    columns = ['threshold','accuracy','precision','recall','F1']\n",
        "    df_metrics = pd.DataFrame(metrics, columns=columns)\n",
        "\n",
        "    return df_metrics"
      ],
      "metadata": {
        "id": "kBQ7dsDEaWF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the thresholds function\n",
        "\n",
        "df_metrics = tune_threshold(y_val, y_pred, 21)\n",
        "df_metrics.plot(x='threshold', y=['accuracy','precision','recall','F1'],\n",
        "                title='model performance on the validation dataset',\n",
        "                kind=\"line\",  style=['+-', 'o-', 'x-', 'd-'], figsize=(20, 5));\n",
        "\n",
        "print('the ROC AUC is', roc_auc_score(y_val, y_pred).round(3))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uD9L9f4FaWDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics[df_metrics.F1.round(1) == max(df_metrics.F1.round(1))].round(3)"
      ],
      "metadata": {
        "id": "Qyrw1JszaV_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_val_metrics(model, X_train, y_train, X_val, y_val, y_pred, t):\n",
        "\n",
        "    val_metrics = []\n",
        "\n",
        "    # TRAIN\n",
        "    y_pred = model.predict_proba(X_train)[:, 1]\n",
        "    acc = accuracy_score(y_train, y_pred >= t)\n",
        "    f1 = f1_score(y_train, y_pred>=t)\n",
        "    rec = recall_score(y_train, y_pred >= t)\n",
        "    auc = roc_auc_score(y_train, y_pred)\n",
        "    print('For the training dataset:',\n",
        "            'ACC:', round(acc,3),'F1:', round(f1,3), 'recall:', round(rec,3), 'ROC AUC:', round(auc,3))\n",
        "\n",
        "    # VAL\n",
        "    y_pred = model.predict_proba(X_val)[:, 1]\n",
        "    val_acc = accuracy_score(y_val, y_pred >= t)\n",
        "    val_f1 =  f1_score(y_val, y_pred>=t)\n",
        "    val_rec = recall_score(y_val, y_pred >= t)\n",
        "    val_auc = roc_auc_score(y_val, y_pred)\n",
        "\n",
        "    val_metrics.append((val_acc, val_f1, val_rec, val_auc))\n",
        "\n",
        "    print('For the validation dataset:',\n",
        "        'ACC:', round(val_acc,3), 'F1:', round(val_f1,3), 'recall:', round(val_rec,3), 'ROC AUC:', round(val_auc,3))\n",
        "\n",
        "    return val_metrics"
      ],
      "metadata": {
        "id": "mGwEWr6cSLah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_val_metrics = calc_val_metrics(LR, X_train, y_train, X_val, y_val, y_pred, t=0.4)"
      ],
      "metadata": {
        "id": "N5o5q6fThryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_val, y_pred >= t)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "ax.xaxis.set_ticklabels(['0','1'])\n",
        "ax.yaxis.set_ticklabels(['0','1'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-SF__zBhrwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM FOREST"
      ],
      "metadata": {
        "id": "Jq-2WdMqh7pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%timeit  # this cells takes some seconds to run\n",
        "\n",
        "# parameters\n",
        "n_estimators = [50, 100, 150]\n",
        "max_depth = [5, 10]\n",
        "min_samples_leaf = [5, 10]\n",
        "\n",
        "RF_param_grid = dict(max_depth = max_depth, min_samples_leaf = min_samples_leaf, n_estimators = n_estimators)\n",
        "\n",
        "# model\n",
        "RF = RandomForestClassifier(max_depth = max_depth,\n",
        "                            min_samples_leaf = min_samples_leaf,\n",
        "                            n_estimators = n_estimators,\n",
        "                            random_state = 1)\n",
        "\n",
        "# metric\n",
        "metric = make_scorer(f1_score)\n",
        "\n",
        "# grid\n",
        "RF_grid = GridSearchCV(\n",
        "        estimator = RF,\n",
        "        param_grid = RF_param_grid,\n",
        "        scoring = metric,\n",
        "        verbose =1)\n",
        "\n",
        "# train the model\n",
        "RF_grid_results = RF_grid.fit(X_train, y_train)\n",
        "\n",
        "# summarize\n",
        "print('Mean F1-score: %.3f' % RF_grid.best_score_)\n",
        "print('Standar Deviation:', RF_grid.cv_results_['std_test_score'][RF_grid.best_index_].round(3))\n",
        "print('Best Parameters: %s' % RF_grid.best_params_)"
      ],
      "metadata": {
        "id": "fDiUDOWNhruI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us extract the best random forest and apply it to the validation set\n",
        "\n",
        "best_RF = RF_grid_results.best_estimator_\n",
        "y_pred = best_RF.predict_proba(X_val)[:, 1]"
      ],
      "metadata": {
        "id": "UDuBmB9Lhrr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the thresholds function\n",
        "\n",
        "df_metrics = tune_threshold(y_val, y_pred, 41)\n",
        "df_metrics.plot(x='threshold', y=['accuracy','precision','recall','F1'],\n",
        "                title='model performance on the validation dataset',\n",
        "                kind=\"line\",  style=['+-', 'o-', 'x-', 'd-'], figsize=(20, 5));\n",
        "\n",
        "print('the ROC AUC is', roc_auc_score(y_val, y_pred).round(3))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F7SZuZJehrpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics[df_metrics.F1.round(3) == max(df_metrics.F1.round(3))].round(3)"
      ],
      "metadata": {
        "id": "lifF75CEiJJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0.4\n",
        "RF_val_metrics = calc_val_metrics(best_RF, X_train, y_train, X_val, y_val, y_pred, t=0.4)"
      ],
      "metadata": {
        "id": "zzHatiOLiJHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=0.4\n",
        "cf_matrix = confusion_matrix(y_val, y_pred >= t)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "ax.xaxis.set_ticklabels(['0','1'])\n",
        "ax.yaxis.set_ticklabels(['0','1'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sy1JSDU4iJFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUACION"
      ],
      "metadata": {
        "id": "QYyPoqogiz-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Model performance on the validation dataset:')\n",
        "\n",
        "df_metrics_metric = pd.DataFrame(['acc', 'F1', 'recall', 'ROC_AUC'])\n",
        "df_metrics_LR = pd.DataFrame(LR_val_metrics).round(3).T\n",
        "df_metrics_RF = pd.DataFrame(RF_val_metrics).round(3).T\n",
        "\n",
        "df_final_val_metrics = pd.concat([df_metrics_metric, df_metrics_LR,\n",
        "                              df_metrics_RF], axis=1)\n",
        "df_final_val_metrics.columns = ['metric', 'LR', 'RF']\n",
        "df_final_val_metrics.set_index('metric')"
      ],
      "metadata": {
        "id": "cwzgxNBNiJCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "\n",
        "# we programatically tuned the main parameters of these models:\n",
        "models.append(('LR', LogisticRegression(max_iter = 50, random_state = 1)))\n",
        "\n",
        "models.append(('RF', RandomForestClassifier(max_depth = 10,\n",
        "                                            min_samples_leaf = 5,\n",
        "                                            n_estimators =  50, random_state = 1)))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "metric = make_scorer(f1_score) # try recall_score if you like\n",
        "\n",
        "print(\"The mean and std of F1-score for the 10-folds cross-validation on the training set:\")\n",
        "print()\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = KFold(n_splits = 10, shuffle = True)\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring= metric)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    print(name, cv_results.mean().round(3), '+-', cv_results.std().round(3))"
      ],
      "metadata": {
        "id": "AyMc2fSDiJAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "YrFKC5mejX62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN\n",
        "df_full_train = df_full_train.reset_index(drop=True) # reset index after splitting shuffling\n",
        "y_full_train = df_full_train['Outcome'].values\n",
        "\n",
        "# 2. scale the numerical features ---------------------------------------------------\n",
        "\n",
        "X_full_train = df_full_train[numericas].values\n",
        "X_full_train = scaler.fit_transform(X_full_train)"
      ],
      "metadata": {
        "id": "B9YYcFZbiI-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJnbjOuBlTBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST\n",
        "df_test = df_test.reset_index(drop=True) # reset index after splitting shuffling\n",
        "y_test = df_test['Outcome'].values\n",
        "\n",
        "del df_test['Outcome'] # remove target\n",
        "\n",
        "\n",
        "# 2. scale the numerical features --------------------------------------------\n",
        "\n",
        "X_test = df_test[numericas].values\n",
        "print(X_test)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "M9K7hm7pjZyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and apply the final model:\n",
        "\n",
        "RF = RandomForestClassifier(max_depth = 10, min_samples_leaf = 5,\n",
        "                            n_estimators =  50, random_state = 1)\n",
        "\n",
        "model = RF.fit(X_full_train, y_full_train)\n",
        "y_pred = RF.predict_proba(X_test)[:, 1]\n",
        "\n",
        "t = 0.4\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred >= t)\n",
        "rec = recall_score(y_test, y_pred >= t)\n",
        "f1  = f1_score(y_test, y_pred >= t)\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "print('For the test dataset:',\n",
        "      'ACC:', round(acc,3), 'F1:', round(f1,3),\n",
        "      'recall:', round(rec,3),'ROC AUC:', round(auc,3))"
      ],
      "metadata": {
        "id": "3piqVAumjZwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_val_metrics = calc_val_metrics(best_RF, X_train, y_train, X_val, y_val, y_pred, t=0.4)"
      ],
      "metadata": {
        "id": "4ivAoKWLjZuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(y_test, y_pred >= t)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
        "\n",
        "ax.set_title('Confusion Matrix\\n');\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "ax.xaxis.set_ticklabels(['0','1'])\n",
        "ax.yaxis.set_ticklabels(['0','1'])\n",
        "\n",
        "##plt.figure(figsize=(7,5))\n",
        "plt.rcParams['figure.figsize'] = [7, 5]  # re-run this cell to get the correct figure size\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B7vRVPtRjZsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_patient = {\n",
        "    'Pregnancies': 1,\n",
        "    'Glucose': 0,\n",
        "    'BloodPressure': 0,\n",
        "    'SkinThickness': 1,\n",
        "    'Insulin': 0,\n",
        "    'BMI': 0,\n",
        "    'DiabetesPedigreeFuntion': 3,\n",
        "    'Age': 34\n",
        " }"
      ],
      "metadata": {
        "id": "ooi57i4LjZp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(list(new_patient.values())).reshape(1, -1)\n",
        "print(X)\n",
        "y_pred = RF.predict_proba(X)[:, 1]\n",
        "print('Model application to new patient:')\n",
        "print()\n",
        "print(\"The patient raw score of suffering a hear failure is:\", y_pred[0].round(2))\n",
        "print()\n",
        "print(\"With t =\", t, \"as the decision threshold, is there a risk of suffering a heart failure?\", y_pred[0] >= t)"
      ],
      "metadata": {
        "id": "iCVkctMjjZnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANCIA DE CADA COLUMNA"
      ],
      "metadata": {
        "id": "1DE25Sc04Uo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = make_scorer(f1_score)\n",
        "\n",
        "result = permutation_importance(RF, X_test, y_test, scoring = metric,\n",
        "                                n_repeats = 10, random_state = 1)\n",
        "RF_per_importances = pd.Series(result.importances_mean, index=feature_names)"
      ],
      "metadata": {
        "id": "M4J4Ke6TjZla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (16, 9))\n",
        "RF_per_importances.plot.bar(yerr=result.importances_std, ax = ax,fontsize = 20)\n",
        "ax.set_title(\"Feature importance by permutation importance on the test set\",fontsize = 20)\n",
        "ax.set_ylabel(\"Mean F1-score decrease\",fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SQyWhbUSjZi-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}